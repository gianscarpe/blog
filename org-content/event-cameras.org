#+STARTUP: indent
#+AUTHOR: Gianluca Scarpellini
#+HUGO_BASE_DIR: ../
#+HUGO_SECTION: posts
#+hugo_auto_set_lastmod: t

* A gentle introduction to event cameras         :introduction:event_cameras:
:PROPERTIES:
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :noauthor true :nocomment true :nodate true
:EXPORT_HUGO_WEIGHT: auto
:EXPORT_FILE_NAME: introduction
:END:
** Introduction
Can computer see like we humans do? Computer vision is a joint branch of machine
learning, statistics, and computer science. It's main goal is providing
artificial agents (e.g., robots, autonomous cars, drones, surveilling systems)
the ability to *see* and *understand* the environment. Deep learning and machine
learning are contributing enormeously to computer vision. Open-source
implementations of the recent literature (e.g., torchvision, tensorflow,
recently opencv) demonstrate that deep learning applications are mature and
ready for industrial applications (#TODO: add examples). The Figure below is an
(abused) example of how fast the community has grown during the past
decade. Today, artificial agent perform better than human on huge set of
classes.
#+attr_html: :width 100% :class center :float right :title Old joke (link https://xkcd.com/1425/)
#+ATTR_ORG: :width 10
[[file:images/old_joke.png]]

Is it all? Has computer vision reached complete maturity? /I believe it's not./

Standard cameras suffer of major intrinsics issues which can hardly be solved -
high bandwidth, high power consumption, low dynamic range. Humans don't see
images at fixed frame rates. We process only the *changes* of our sourrindings,
avoiding to process again and again the statical part of the scene. Our approach
is faster and more efficient, as we don't process the same information multiple
times.

I praise the human vision system because Nature has always been a source of
inspiration for engineering. Could we rethink computer vision, taking source
from Nature?g *Event-cameras* (cameras based on *events*) could be the
answer. In the following paragraphs, I try to convince the reader of the
potentials of event-based vision applications. This article is structure as
follows. I present *event-cameras*, their *principle of operations*, and their
motivation. Next, I show some solutions to solve the *main challenges of
event-based vision*. And finally, I conclude with a what I believe its the key:
everyone can contribute.

** Event-cameras
Traditional cameras collect light and, at fixed frame-rate, output a single
frame. This approach suffers, however, of severe limitations. If part of the
scene is static, each frame contains the *same* pixel information, and the data
becomes *redundant*. Consequently, standard cameras with high frame-rate produce a
*prohibitive large amount of throughput*---most of which redundant if no change
occurs---that no algorithm can efficiently process. These limitation leads to
other common issues in standard computer vision: narrowed dynamic range and
blooming.
#+attr_html: :class center :float right :title GoPro vs Prophesee event-camera
#+ATTR_ORG: :width 100
[[file:images/gopro.gif]]


Each standard image is *digitalized* and stored with a finite range of values
(e.g., 256 values per channel). If the light varies abruptly in the scene, the
digitalization process loses much of the information to fit into the fixed
range. Consequently, traditional cameras have a narrow dynamic range, especially
in automotive applications. For example, the sun in front of the driver is a
problematic issue for a standard camera. Moreover, if the incoming light is too
much, as in the after-mentioned example, the pixels saturate and the resulting
image is *burned*.

The amateur photographer is aware of these issues and can use these
characteristics to create more artistic photo. On the other hand, computer
vision algorithms must be *reliable*, not artistic, and their reliability is
under attack in dynamic scenarios or when sources of lights are present in the
scene.
#+attr_html: :class center :float right :title This is me saying \"hello\" at an event-camera
#+ATTR_ORG: :width 100
[[file:images/me.png]]

*Can we rethink computer vision?*

This question lead us to the central idea of event-cameras and its
paradigm. Event-cameras capture *change of intensities* (also called events) in
an asynchronous manner. *Each event has 4 values: the position tuple on the
image plane (x, y), the polarity of the change (positive or negative), and the
timestamp of the its occurrence*. This paradigm shift leads to some interesting
advantages.

First of all, since each pixel work independently, event-cameras are more robust
against strong light changes. When the pixel of a standard camera receives too
much light intensity (e.g., if a camera is mounted on a car heading west at
dawn), it "flows" into the neighborhood pixels (*blooming effect*). On the other
hand, pixels of event-cameras don't store intensity values: events are spiked
immediately when a threshold is reached. No blooming happens, and dynamic range
is extremely high (140 dB vs 60 dB of traditional cameras). As you can imagine,
if each pixel transmit *only* the intensity changes, the bandwidth---the amount
of transmitted data---stays very low, while the rate---the speed of data
transmission---grows over 1 million events per second. Consequently, events
paradigm is for vision what oil is for energy: efficient, rapid, and
flexible. In the following Figure from cite:Gallego_2020 you can find an example
of what I'm saying.
#+attr_html: :class center :float right :title Events vs frames 
#+ATTR_ORG: :width 100
[[file:images/events.png]]

These interesting characteristics made event-cameras the best available solution
for robotics platform, intrusion detection, and fast movement tracking. Events
data preserve privacy of the monitored space as well, because no subjects'
details (e.g., face, eyes, and so on) are captured. Major companies [fn::
Samsung [[https://www.samsung.com/au/smart-home/smartthings-vision-u999/]]] are
exploring low-cost event-based applications for intrusion detection.

** Where do the disadvantages lay?

Event-cameras detect only the movements and transmit asynchronous data. If the
scene is static, like in a beautiful mountain panorama, the only events are
noise. This new way of imaging makes events data incompatible with existing
computer vision applications. Moreover, deep learning models rely on
*synchronous* tensor representation, therefore even the *deep learning
revolution* is not directly available for event cameras.

** Solutions
If you were looking for the Saint Grail of computer vision, look somewhere
else. They are not. Event-cameras are novel, expensive sensors---especially if
you are a first-time practitionares in computer vision. A huge effort is still
required to unlock their potential. I want to send this message clearly:
everyone can participate. Research in computer vision---and in many other fields
of compute science---needs fresh ideas. What makes event-cameras hard to employ
also makes them *exciting*---we are explorer looking for new, more efficient
paths.

What do we need?

We need *data*. How could we exploit event-cameras otherwise? Dataset collection
is boring and hard, but necessary. There are other, more interesting paths worth
noting. For example, *domain adaptation* could be worth some consideration. We
could train on simulated dataset; *if it doesn't work in simulation, it doesn't
work in the real world real*. Simulation could be a game changer in event-based
vision; we need standardize, easy-to-use simulators. They must be *open-source*
as everyone could contribute.

Scaramuzza and his team developed an open-source simulator
cite:Rebecq2018ESIM:Simulator. This could be exploit to generate huge amount of
publicly available dataset, both with complete simulation or by converting
standard computer vision dataset to events data. They made their point in an
elegant way: they engineered an approach for *reconstructing* gray-scale images
from events cite:Rebecq19pami. Their reconstruction is smooth and clear even in
impossible conditions---e.g., capturing a bullet moving at 1,000 km/h as in the
following Figure. This should be clear by now: they could only achieve this
amazing reconstruction quality using *simulated data* for training.
#+attr_html: :class center :width 2000px :float right :title Triple-shot dwarf (P10 camera, event-camera, and with a real gun)
#+ATTR_ORG: :width 200
[[file:images/dwarf.png]]


We can group events together and use events as standard frames. Events frames
can be stacked in tensors and used as input for synchronous deep learning models
(e.g., CNN, ANN). Currently, representing events as frames is the most
common technique to exploit event-cameras. This approach increases, however,
bandwidth and redundancy, and its benefits are limited compared to standard
cameras. Asynchronous machine learning models---e.g., Spiking Neural
Network---and sparse CNN cite:MessikommerEvent-basedNetworks are fascinating
path of research.

** Conclusion
I presented event-cameras and showed their advantages and their limits. I
discuss how and in what measures event-cameras could change computer vision
applications. In particular, I focused on application that necessitates low
bandwidth, high acquisition rates, and low power consumption. The range of
applications of event-camera is a broad one: from space applications, in which
low energy consumption is the key to pack lighter batteries and ultimately save
millions on budget, to autonomous driving, where blurring and overflowing are
still dangerous for the driver. The path toward exploiting event-based cameras
is clear. Realistic and easy-to-use simulators could save time and energy in
collecting data. A well-engineered event-based vision package, lets call it
OpenEV, would certaintly increase the interest of industries and practitionares,
especially if big players were backing the project (we'll see, Samsung and Sony
have already shown their interest). If I made you more curious, kudos to me! I
refer you to cite:Gallego_2020 ---a fantastic and exhaustive overview of
event-cameras overview---and to [[http://rpg.ifi.uzh.ch/docs/scaramuzza/2019.07.11_Scaramuzza_Event_Cameras_Tutorial.pdf][Davide Scaramuzza's tutorial on event-cameras]].

* Lifting Monocular Events to 3D Human Poses :event_cameras:publications:CVPRw:
:PROPERTIES:
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :noauthor true :nocomment true :nodate true :nopaging true :noread true
:EXPORT_HUGO_WEIGHT: auto
:EXPORT_FILE_NAME: scarpellini2021lifting
:END:
#+attr_html: :width 100% :class center :float right :title The problem: from a moving subject  to skeleton estimation
#+ATTR_ORG: :width 10
[[file:images/litftingscarpellini_abstr.png]]


** Abstract
This paper presents a novel 3D human pose estimation approach using a single
stream of asynchronous events as input. Most of the state-of-the-art approaches
solve this task with RGB cameras, which suffer when the subjects are moving
fast. On the other hand, event-based 3D pose estimation benefits from the
advantages of event-cameras, especially their efficiency and robustness to
appearance changes. Yet, finding human poses in asynchronous events is in
general more challenging than standard RGB pose estimation, since little or no
events are triggered in static scenes. Here we propose the first learning-based
method for 3D human pose from a single stream of events. Our method consists of
two steps. First, we process the event-camera stream to predict three orthogonal
heatmaps per joint; each heatmap is the projection of of the joint onto one
orthogonal plane. Next, we fuse the sets of heatmaps to estimate 3D localisation
of the body joints. As a further contribution, we make available a new,
challenging dataset for event-based human pose estimation by simulating events
from the RGB Human3.6m dataset. Experiments demonstrate that our method achieves
state-of-the-art accuracy, narrowing the performance gap between standard RGB
and event-based vision.
- [[https://github.com/gianscarpe/event-based-monocular-hpe][Code]]
- [[https://iit-pavis.github.io/lifting_events_to_3d_hpe/][Webpage]]
- [[https://arxiv.org/abs/2104.10609][Arxiv]]

** BibTex citation
#+begin_example
@article{scarpellini2021lifting,
  title={Lifting Monocular Events to 3D Human Poses},
  author={Scarpellini, Gianluca and Morerio, Pietro and Del Bue, Alessio},
  journal={arXiv preprint arXiv:2104.10609},
  year={2021}
}
#+end_example
* Some insights from working with event-cameras for Human Pose Estimation :event_cameras:
:PROPERTIES:
:EXPORT_HUGO_CUSTOM_FRONT_MATTER: :noauthor true :nocomment true :nodate true :nopaging true :noread true
:EXPORT_HUGO_WEIGHT: auto
:EXPORT_FILE_NAME: insights_event_hpe_research
:END:
As I write, [[https://iit-pavis.github.io/lifting_events_to_3d_hpe/][my paper about event-cameras and human-pose estimation]] has been
accepted at the Computer Vision and Pattern Recognition workshop on
event-cameras[fn::Link at https://tub-rip.github.io/eventvision2021/]. If you do
not know what an event-camera is, I wrote about them [[file:event-cameras.org::*A gentle introduction to event cameras][a brief introduction]].
Instead, in this post I just want to share some advice I found useful during my
research, hopefully to inspire curiosity in other researchers, practitioners,
and entrepreneurs.

#+attr_html: :width 100% :class center :float right :title The problem: from a moving subject  to skeleton estimation
#+ATTR_ORG: :width 10
[[file:images/litftingscarpellini_abstr.png]]

** Open-source research
I'm convinced that Computer vision researchers and practitionares should direct
each other to increase the state-of-the-art in meaningful ways. To reach such a
goal, researchers should take a step down the ivy-tower. How? I believe the
first steps are *writing good-quality code* and *summarizing paper for the general
public*.

Open-source doesn't mean *low-quality*. Event-based vision is novel, very novel;
*the opencv for event-cameras* doesn't exist yet, although some are working onto
it. The Robotics and Perception Group (the university of Zurich and ETH Zurich)
is collecting papers implementation on their [[https://github.com/uzh-rpg/event-based_vision_resources][github page]]. [[https://www.prophesee.ai/][Prophesee]]---a really
cool private company---already has a SDK for their event-cameras, but the
software is limited to its products.

As an attempt, I developed my own tiny library with some tools for
event-cameras. You can find it at https://github.com/IIT-PAVIS/event_library.

** Event-synthesis
*Data* are the fuel of novel Deep Learning models. Continuing with the analogy to
oil, data are also extremely expensive to extract and elaborate. In particular,
datasets recorded with event-cameras are not comparable in size and variance to
the large RGB datasets. For these reasons, *event synthesis* is amazing. Recent
literature provides interesting results about generate synthetic events from RGB
images or videos cite:Rebecq19pami.

I used my /event-library/ to generate synthetic events from a millions of RGB
frames of the standard Human3.6m dataset cite:ionescu14_human.

#+attr_html: :width 100% :class center :float right :title Syntehtic events allow to recycle standard RGB datasets and test algorithms without using an event-camera
#+ATTR_ORG: :width 10
[[file:images/h3m.png]]

I find this especially intriguing for entrepreneurs and engineers in automotive
and time-critical applications. Companies can convert their own data to
synthetic events in order to evaluate event-based algorithms before event
acquiring event-cameras (which are still expensive, although major companies
entering the field will lower costs).

** My research -- what about Human Pose Estimation?
#+begin_quote
/Only in code veritas./
#+end_quote
You have certainly appreciated the results of "Human Pose Estimation" (also
called "Motion Capture") techniques. They are exploited in /Star Wars/, /Avatar/,
and major video-games to produce amazing special effect (some examples
https://www.youtube.com/watch?v=bkvW9hsypHw).

Motion Capture systems are based on specialize hardware and a large number of
synchronized cameras. In this conditions, motions at high speed are hard to
capture. My research looks for an answer to a common problem in these
applications: *elaborating data at high speed*.

#+attr_html: :width 100% :class center :float right :title Motion Capture with an event-camera
#+ATTR_ORG: :width 10
[[file:images/liftingscarpellini_results.png]]

Event-cameras record rapid movements efficiently. These devices could be
disruptive in motion capture industries, *but first there are some major issues
to tackle*:
- First, static parts of the body generate no events (as in fig. above). This is
  a major issue, as no information means lower reconstruction accuracy. In this
  case, event-cameras needs some backup from standard RGB cameras
- Second, more /smart/ techniques should be proposed to leverage the uniqueness of
  events. In my [[file:event-cameras.org::*A gentle introduction to event cameras][previous post about event-cameras]], I explore recent literature
  related to /event-by-event/ and /group-of-events/ approaches cite:Gallego_2020.

* How to calibrate an event-camera?

* COMMENT Notes
   - WHO: Open post about event-cameras to attract researchers and computer
     vision practitioners 
   - WHY: more people working with event-cameras, more data, more cooperation,
     and more fun!
   - *Central idea*: present event-cameras and send a message to the computer
     vision community; join us, and help us build the next opencv for
     event-cameras
